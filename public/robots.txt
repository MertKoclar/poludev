# robots.txt - Poludev Web Sitesi
# https://poludev.com/robots.txt
# Bu dosya arama motoru botlarına hangi sayfaların taranabileceğini ve hangilerinin taranamayacağını söyler
# This file tells search engine bots which pages can be crawled and which cannot

# Tüm botlar için genel kurallar - General rules for all bots
User-agent: *
Allow: /
Disallow: /admin/
Disallow: /login
Disallow: /api/

# Sitemap konumu - Sitemap location
Sitemap: https://poludev.com/sitemap.xml

# Tarama gecikmesi (saniye) - Crawl delay (seconds)
# Botların sunucuya çok fazla yük bindirmemesi için bekleme süresi
# Wait time to prevent bots from putting too much load on the server
Crawl-delay: 1

# Özel botlar için izinler - Permissions for specific bots

# Google Bot
User-agent: Googlebot
Allow: /
Crawl-delay: 1

# Bing Bot
User-agent: Bingbot
Allow: /
Crawl-delay: 1

# Yahoo Bot
User-agent: Slurp
Allow: /
Crawl-delay: 1

# DuckDuckGo Bot
User-agent: DuckDuckBot
Allow: /
Crawl-delay: 1

# Baidu Bot (Çin arama motoru)
User-agent: Baiduspider
Allow: /
Crawl-delay: 2

# Yandex Bot (Rus arama motoru)
User-agent: YandexBot
Allow: /
Crawl-delay: 1

# Sogou Bot (Çin arama motoru)
User-agent: Sogou
Allow: /
Crawl-delay: 2

# Exabot
User-agent: Exabot
Allow: /
Crawl-delay: 1

# Facebook Bot
User-agent: facebot
Allow: /
Crawl-delay: 1

# Internet Archive Bot
User-agent: ia_archiver
Allow: /
Crawl-delay: 1

# Kötü niyetli botları engelle - Block malicious bots
User-agent: AhrefsBot
Disallow: /

User-agent: SemrushBot
Disallow: /

User-agent: MJ12bot
Disallow: /

User-agent: DotBot
Disallow: /
